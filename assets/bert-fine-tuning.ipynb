{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/lsoica/blog/blob/main/assets/bert-fine-tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# BERT\n",
    "\n",
    "BERT is a transformer model developed for fine tuning on NLP tasks.\n",
    "\n",
    "Use case:\n",
    "- Masked language modeling: The cat [MASK] on the mat. -> BERT -> The cat sat on the mat.\n",
    "- Next sentence prediction: [CLS] The cat sat on the mat. [SEP] The dog slept on the rug. [SEP] -> BERT -> True\n",
    "- Text classification: Assign a label to a text sequences: An email is spam or not spam. Categorize IT tickets into one of the classes.\n",
    "\n",
    "![Architecture](images/bert-architecture.png)\n",
    "\n",
    "## Training a BERT model\n",
    "\n",
    "When during the training of a BERT model, the train loss is decreasing but the validation loss is increasing, it means that the model is overfitting. This means that the model is learning the training data too well and is not generalizing to unseen data.\n",
    "\n",
    "Pooler: BERT has a pooling layer that takes the output of the last layer and compresses it into a vector of fixed size.\n",
    "\n",
    "## Fine tuning\n",
    "\n",
    "What is fine tuning? Taking a pre-trained model and training (some of) its parameters on a new task.\n",
    "When to fine tune?\n",
    " - Plain prompt engineering is not working.\n",
    " - A smaller fine-tuned model can outperform a larger one.\n",
    "\n",
    " How?\n",
    "  - Self-supervised\n",
    "  - Supervised\n",
    "    - Choose fine-tuning task\n",
    "    - Prepare dataset\n",
    "    - Choose a base model\n",
    "    - Fine-tune via supervised learning\n",
    "    - Evaluate\n",
    "    - Deploy\n",
    "  - Reinforcement learning\n",
    "\n",
    "Parameter training options:\n",
    "  - Retrain all parameters\n",
    "  - Transfer learning: Retrain only the last layer parameters.\n",
    "  - Parameter-efficient fine-tuning: Retrain only a small portion of the parameters.\n",
    "   - LoRA: Low-rank adaptation: Add trainable rank-2 matrices to the model.\n",
    "\n",
    "## Fine tuning BERT with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate peft torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "untrained model predictions:\n",
      "--------------------\n",
      "I love this movie -> Negative\n",
      "I hate this movie -> Negative\n",
      "Not a fan, don't recommend -> Negative\n",
      "This one is a pass -> Negative\n",
      "trainable params: 628,994 || all params: 67,584,004 || trainable%: 0.9307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/59/c32_bthx48jd9m2ym5m3tnpw0000j7/T/ipykernel_19744/1863821626.py:78: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [01:47<00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30286046862602234, 'eval_accuracy': {'accuracy': 0.891}, 'eval_runtime': 34.387, 'eval_samples_per_second': 29.081, 'eval_steps_per_second': 7.27, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [01:48<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 108.427, 'train_samples_per_second': 9.223, 'train_steps_per_second': 2.306, 'train_loss': 0.4968539123535156, 'epoch': 1.0}\n",
      "trained model predictions:\n",
      "--------------------\n",
      "I love this movie -> Positive\n",
      "I hate this movie -> Negative\n",
      "Not a fan, don't recommend -> Negative\n",
      "This one is a pass -> Negative\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "\n",
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\": 0, \"Positive\": 1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, \n",
    "                                                           num_labels=2, \n",
    "                                                           id2label=id2label, \n",
    "                                                           label2id=label2id,).to(\"mps\")\n",
    "\n",
    "dataset_dict = load_dataset(\"shawhin/imdb-truncated\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    text = examples[\"text\"]\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(text, truncation=True, return_tensors=\"np\", max_length=512)\n",
    "    return tokenized_inputs\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}\n",
    "\n",
    "text_list = [\"I love this movie\", \"I hate this movie\", \"Not a fan, don't recommend\", \"This one is a pass\"]\n",
    "\n",
    "print(\"untrained model predictions:\")\n",
    "print(\"-\"*20)\n",
    "\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"mps\")\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.argmax(logits)\n",
    "    print(f\"{text} -> {id2label[predictions.tolist()]}\")\n",
    "\n",
    "# Now let's fine-tune the model with LoRA\n",
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\", r=4, lora_alpha=32, target_modules=[\"q_lin\"], lora_dropout=0.01)\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epochs = 1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_checkpoint + \"-lora-text-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"trained model predictions:\")\n",
    "print(\"-\"*20)\n",
    "\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"mps\")\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.argmax(logits)\n",
    "    print(f\"{text} -> {id2label[predictions.tolist()]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning for phishing link detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import DatasetDict, Dataset, load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = load_dataset(\"shawhin/phishing-site-classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "model_path = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "id2label = {0: \"Safe\", 1: \"Not Safe\"}\n",
    "label2id = {\"Safe\": 0, \"Not Safe\": 1}\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, \n",
    "                                                           num_labels=2, \n",
    "                                                           id2label=id2label, \n",
    "                                                           label2id=label2id,).to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze base model parameters\n",
    "for name, param in model.base_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# unfreeze base model pooling layers\n",
    "for name, param in model.base_model.named_parameters():\n",
    "    if \"pooler\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define text preprocessing\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = dataset_dict.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "auc_score = evaluate.load(\"roc_auc\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # get predictions\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # apply softmax to get probabilities\n",
    "    probabilities = np.exp(predictions) / np.exp(predictions).sum(-1, keepdims=True)\n",
    "    # use probabilities of the positive class for ROC AUC\n",
    "    positive_class_probs = probabilities[:, 1]\n",
    "    # compute auc\n",
    "    auc = np.round(auc_score.compute(prediction_scores=positive_class_probs, references=labels)['roc_auc'],3)\n",
    "    \n",
    "    # predict most probable class\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    # compute accuracy\n",
    "    acc = np.round(accuracy.compute(predictions=predicted_classes, references=labels)['accuracy'],3)\n",
    "    \n",
    "    return {\"Accuracy\": acc, \"AUC\": auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hyperparameters\n",
    "lr = 2e-4\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert-phishing-classifier_teacher\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/c32_bthx48jd9m2ym5m3tnpw0000j7/T/ipykernel_69699/2732273287.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      " 10%|â–ˆ         | 263/2630 [00:21<03:28, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4922, 'grad_norm': 1.1272960901260376, 'learning_rate': 0.00018, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|â–ˆ         | 263/2630 [00:26<03:28, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4166680574417114, 'eval_Accuracy': 0.787, 'eval_AUC': 0.912, 'eval_runtime': 4.9842, 'eval_samples_per_second': 90.285, 'eval_steps_per_second': 11.436, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 526/2630 [00:47<02:45, 12.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3905, 'grad_norm': 2.191509485244751, 'learning_rate': 0.00016, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|â–ˆâ–ˆ        | 526/2630 [00:52<02:45, 12.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3610435724258423, 'eval_Accuracy': 0.813, 'eval_AUC': 0.931, 'eval_runtime': 5.0556, 'eval_samples_per_second': 89.01, 'eval_steps_per_second': 11.275, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 789/2630 [01:14<02:28, 12.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3854, 'grad_norm': 0.820168137550354, 'learning_rate': 0.00014, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 789/2630 [01:19<02:28, 12.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31684166193008423, 'eval_Accuracy': 0.858, 'eval_AUC': 0.939, 'eval_runtime': 4.8727, 'eval_samples_per_second': 92.351, 'eval_steps_per_second': 11.698, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1052/2630 [01:40<01:44, 15.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3592, 'grad_norm': 1.5052204132080078, 'learning_rate': 0.00012, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1052/2630 [01:44<01:44, 15.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4752054810523987, 'eval_Accuracy': 0.793, 'eval_AUC': 0.942, 'eval_runtime': 4.7917, 'eval_samples_per_second': 93.913, 'eval_steps_per_second': 11.896, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1315/2630 [02:05<01:20, 16.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3511, 'grad_norm': 3.0983104705810547, 'learning_rate': 0.0001, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1315/2630 [02:10<01:20, 16.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33138200640678406, 'eval_Accuracy': 0.86, 'eval_AUC': 0.946, 'eval_runtime': 4.6846, 'eval_samples_per_second': 96.059, 'eval_steps_per_second': 12.167, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1578/2630 [02:31<01:13, 14.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3536, 'grad_norm': 2.456183671951294, 'learning_rate': 8e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1578/2630 [02:36<01:13, 14.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30289924144744873, 'eval_Accuracy': 0.871, 'eval_AUC': 0.948, 'eval_runtime': 4.9382, 'eval_samples_per_second': 91.126, 'eval_steps_per_second': 11.543, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1841/2630 [02:57<00:47, 16.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3196, 'grad_norm': 2.2076616287231445, 'learning_rate': 6e-05, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1841/2630 [03:02<00:47, 16.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2912053167819977, 'eval_Accuracy': 0.862, 'eval_AUC': 0.949, 'eval_runtime': 5.0116, 'eval_samples_per_second': 89.791, 'eval_steps_per_second': 11.374, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2104/2630 [03:24<00:41, 12.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3285, 'grad_norm': 4.401841163635254, 'learning_rate': 4e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2104/2630 [03:28<00:41, 12.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29782968759536743, 'eval_Accuracy': 0.876, 'eval_AUC': 0.949, 'eval_runtime': 4.7566, 'eval_samples_per_second': 94.605, 'eval_steps_per_second': 11.983, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2367/2630 [03:50<00:29,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3152, 'grad_norm': 0.3482799828052521, 'learning_rate': 2e-05, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2367/2630 [03:54<00:29,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28831204771995544, 'eval_Accuracy': 0.864, 'eval_AUC': 0.951, 'eval_runtime': 4.5704, 'eval_samples_per_second': 98.461, 'eval_steps_per_second': 12.472, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2630/2630 [04:16<00:00, 16.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3053, 'grad_norm': 4.398026943206787, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2630/2630 [04:21<00:00, 16.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2977932393550873, 'eval_Accuracy': 0.871, 'eval_AUC': 0.951, 'eval_runtime': 4.7019, 'eval_samples_per_second': 95.706, 'eval_steps_per_second': 12.123, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2630/2630 [04:22<00:00, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 262.2333, 'train_samples_per_second': 80.081, 'train_steps_per_second': 10.029, 'train_loss': 0.36006521942950925, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2630, training_loss=0.36006521942950925, metrics={'train_runtime': 262.2333, 'train_samples_per_second': 80.081, 'train_steps_per_second': 10.029, 'total_flos': 706603239165360.0, 'train_loss': 0.36006521942950925, 'epoch': 10.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:04<00:00, 12.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': np.float64(0.864), 'AUC': np.float64(0.951)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# apply model to validation dataset\n",
    "predictions = trainer.predict(tokenized_data[\"test\"])\n",
    "\n",
    "# Extract the logits and labels from the predictions object\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Use your compute_metrics function\n",
    "metrics = compute_metrics((logits, labels))\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Infer on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/c32_bthx48jd9m2ym5m3tnpw0000j7/T/ipykernel_69699/3933651874.py:17: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  probabilities = np.exp(outputs.logits) / np.exp(outputs.logits).sum(-1, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe google.com\n",
      "Safe yahoo.com\n",
      "Safe www.yahoo.com\n",
      "Safe https://www.yahoo.com\n",
      "Not Safe https://microsoft.user-account.online/14e84edd29dc7302?l=861\n",
      "Not Safe users11.jabry.com/reaseo/Aolupdate.htm\n",
      "Not Safe www.allandmedia.com/opencart/system/Cielo/index.html\n",
      "Not Safe mrterabit.com/remax/index.php\n",
      "Not Safe phishing.org\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "  \"google.com\",\n",
    "  \"yahoo.com\",\n",
    "  \"www.yahoo.com\",\n",
    "  \"https://www.yahoo.com\",\n",
    "  \"https://microsoft.user-account.online/14e84edd29dc7302?l=861\",\n",
    "  \"users11.jabry.com/reaseo/Aolupdate.htm\",\n",
    "  \"www.allandmedia.com/opencart/system/Cielo/index.html\",\n",
    "  \"mrterabit.com/remax/index.php\",\n",
    "  \"phishing.org\"\n",
    "]\n",
    "for url in urls:\n",
    "  inputs = tokenizer(url, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    outputs = trainer.model.forward(**inputs)\n",
    "    probabilities = np.exp(outputs.logits) / np.exp(outputs.logits).sum(-1, keepdims=True)\n",
    "    print(\"Safe\" if probabilities[0][0].item() > 0.9 else \"Not Safe\", url)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[BERT - Fine tuning for phishing link detection](https://www.youtube.com/watch?v=4QHg8Ix8WWQ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
