{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation)\n",
    "\n",
    "## Terms\n",
    "\n",
    "- Fine-tuning: Train an existing model on new data. It is expensive and limited by the number of parameters of teh base model. It is not additive. It could forget exiting knowledge and replace it with new knowledge.\n",
    "- Few-shot prompting: Teach a LM to perform a new task by first providing with some examples. Then, the model is asked to generate new examples.\n",
    "- Retrieval: Find relevant documents from a large corpus of documents.\n",
    "\n",
    "## RAG pipeline\n",
    "\n",
    "We start with a base model. Our goal is to extend the model for QA on data that was not train on, and without the user having to look for the answers in external sources and provide it as context to the model.\n",
    "\n",
    "- Identify the data source(s): PDF documents, websites, any other document source.\n",
    "- Split the document(s) into chunks\n",
    "- For each chunk, create embeddings for each chunk. An embedding is a vector of fixed size that represents the meaning of the chunk.\n",
    "- Store the pairs of embedding, chunk in a vector database.\n",
    "- At query time, we retrieve the relevant chunks from the vector database by searching for the most similar embeddings to the embedding of the query.\n",
    "- Feed the most similar chunks into the base model's context and ask it to generate answers.\n",
    "\n",
    "## Embedding vectors\n",
    "\n",
    "Multi-dimensional vector representations of chunks or words or any other meaningful unit. After training, the embeddings end up being able to capture semantic relationships between chunks:\n",
    " - we expect that the angle between words with similar meaning to be small (the vectors are close together, similar) and that the angle between words with different meanings will be large (the vectors are far apart).\n",
    "  - The cosine similarity is a common way to measure the similarity (angle) of two vectors, based on the dot product of the 2 vectors.\n",
    "\n",
    "## Embedding models\n",
    "\n",
    "These are models that can be used to generate embedding vectors for sentences, chunks or words. An example is the Sentence BERT model. The model generates embedding vectors for each of the tokens in the context. Then, the vectors are averaged together into a single vector that can then be used in cosine similarity with the embedding vector of the query. BERT has by default 768 dimensions. In order to represent an embedding in a lower dimensionality, we use a linear layer right after applying the average, that maps each of these 768 dimensions to a smaller number of dimensions. We call this number of dimensions the embedding size.\n",
    "\n",
    "## Vector database\n",
    "\n",
    "Stores vectors of fixed dimensions (embeddigns) so that later on we can query the DB fo the most similar embeddings to a given embedding vector. A common way is to use an approximate KNN (nearest neighbour) search algorithm, like Annoy or Faiss. These algorithms are used to find the closest embeddings in a vector database. This is true not only for text, buat audio, images and other data as well.\n",
    "\n",
    "## K-NN search\n",
    "\n",
    "With a naive approach, we would have to compute the cosine similarity of each embedding with every other embedding. An optimization to this is the HNSW that stands for Hierarchical Navigable Small Worlds. It uses a graph structure to store the embeddings. The graph is built using a nearest neighbour search algorithm (e.g., Annoy or Faiss). This way, we can query the graph for the closest embedding and then traverse the graph to find the closest neighbors of that embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG pipeline with LlamaIndex and Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index llama-index-llms-ollama bs4 requests llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Claude 3.5 Sonnet is not a well-known or established literary reference, and I couldn\\'t find any information on it from reputable sources.\\n\\nHowever, I did come across something interesting. The \"Claude\" in question might be related to a person named Claude Cahun, a French photographer, artist, and writer. In 1928, she published a book of self-portraits titled \"Aveux non avenus\" (Confessions Unfiltered), which included a series of sonnets.\\n\\nIt\\'s possible that you are thinking of one of these sonnets from Claude Cahun\\'s work. If this is the case, I\\'d be happy to try and help you understand the context and meaning behind them!\\n\\nIf not, please provide more information or context about the \"Claude 3.5 Sonnet\" you\\'re referring to.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.1:8b\", request_timeout=600)\n",
    "response = llm.complete(\"Do you know about Claude 3.5 Sonnet?\")\n",
    "\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from llama_index.core import Document\n",
    "\n",
    "urls = [\n",
    "    \"https://www.anthropic.com/news/claude-3-5-sonnet\",\n",
    "    \"https://www.reddit.com/r/ClaudeAI/comments/1dvfyp6/all_this_talk_about_claude_sonnet_35_being_good/\"\n",
    "]\n",
    "\n",
    "docs = []\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    docs.append(Document(text=soup.text, metadata={\"source\": url}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.llm = Ollama(model=\"llama3.1:8b\", request_timeout=600)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(docs)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "response = query_engine.query(\"Do you know about Claude 3.5 Sonnet?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude 3.5 Sonnet is a new model that has been released, it's described as setting new industry benchmarks for certain types of evaluations. It's also mentioned to operate at twice the speed of an older model and is suitable for complex tasks like context-sensitive customer support and orchestrating multi-step workflows.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'524adcda-71ac-49f3-b826-3ec1f75c9d21': {'source': 'https://www.anthropic.com/news/claude-3-5-sonnet'},\n",
       " '3e950c9e-dd25-48a3-b841-ce1ea86bf6b4': {'source': 'https://www.reddit.com/r/ClaudeAI/comments/1dvfyp6/all_this_talk_about_claude_sonnet_35_being_good/'}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
