{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/lsoica/blog/blob/main/assets/llama-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# LLaMA\n",
    "\n",
    "## Architecture\n",
    "\n",
    "![Architecture](images/transformer_vs_llama_architecture.png)\n",
    "\n",
    "- It is a decoder-only model. So the decoder part of the original transformer paper minus the cross-attention block.\n",
    "- All the normalizations have been moved before every block (the RMS Norm blocks).\n",
    "- The decoder is a transformer, but it's not used in inference (it's just there to train the model\n",
    "- Positional Encodings have become the Rotary Positional Encodings and they are only applied to the Query and Key, but not the Value. They come into play when we calculate the attention. \n",
    "- The attention is a self-attention block, with KV cache.\n",
    "- ReLU have been replaced by SwiGLU.\n",
    "- Embeddings: for each token, a vector of 4096 dimensions. These are trainable paremeters.\n",
    "- Normalization: let's consider we have 2 layers l1 -> L2. If after the backpropagation the output of L1 changes drastically, then the output of L2 will also change drastically. This is not good for training, it makes it slow as the neurons are forced to change drastically their weights in one direction or another. To avoid this, we use Layer Normalization. How it works? For each input item, the mean and variance are calculated and maintained. Then, the output is normalized using the mean and variance of that item.\n",
    "- Batch normalization: for batch normalization, we normalize by columns (features)\n",
    "- Layer normalization: for layer normalization, we normalize by rows (data items). Recenters around 0 mean and rescales to 1 standard deviation.\n",
    "- RMSNorm: Root Mean Square Normalization. It doesn't recenter but just rescale. Less computation compared to LayerNorm.\n",
    "- Positional Encoding: a vector that is summed to the embedding of each token. It indicates the position of the token inside the sentence. They are not learned. Computed once and then used during both training and inference.\n",
    "- Rotary Positional Encoding: it's like positional encoding but instead of absolute position encoding, it stores the distance between 2 token's position\n",
    "- Self-Attention: $Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ . This resulting attention matrix captures not only the meaning and possition of each token but also its relation to other tokens.\n",
    "- KV Cache: avoid redundant computations for each new token and instead cache the previously computed keys and values and append them to the current keys and values.\n",
    "- Multi-Query Attention: Problem: GPUs are very fast at computing, but not so fast at transfering data to/from their memory. \n",
    "- Grouped Multi-Query Attention: We divide the queries into groups and for each group we have one different head. Loss in quality but computationally fast. ![Attention](images/grouped-multi-query-attention.png)\n",
    "- FeedForward SwiGLU: The activation function instead of ReLU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (2.5.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: tqdm in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (4.66.5)\n",
      "Requirement already satisfied: filelock in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.1)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch sentencepiece tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2544  100  2544    0     0   4678      0 --:--:-- --:--:-- --:--:--  4676\n",
      "Enter the URL from email: ^C\n"
     ]
    }
   ],
   "source": [
    "# Download the script\n",
    "!curl -O https://raw.githubusercontent.com/meta-llama/llama/refs/heads/main/download.sh\n",
    "\n",
    "# Make the script executable\n",
    "!chmod +x download.sh\n",
    "\n",
    "# Execute the script\n",
    "#!./download.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import math\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A class to store the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = -1 # set when loading the tokenizer\n",
    "    multiple_of: int = 256\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "    device: str = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_theta_pos_frequencies(head_dim: int, seq_len: int, device: str, theta: float = 10000.0):\n",
    "    # As written in the paragraph 3.2.2 of the paper\n",
    "    # >> In order to generalize our results in 2D to any xi ∈ Rd where **d is even**, [...]\n",
    "    assert head_dim % 2 == 0, \"Dimension must be divisible by 2\"\n",
    "    # Build the theta parameter\n",
    "    # According to the formula theta_i = 10000^(-2(i-1)/dim) for i = [1, 2, ... dim/2]\n",
    "    # Shape: (Head_Dim / 2)\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "    # Shape: (Head_Dim / 2)\n",
    "    theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device) # (Dim / 2)\n",
    "    # Construct the positions (the \"m\" parameter)\n",
    "    # Shape: (Seq_Len)\n",
    "    m = torch.arange(seq_len, device=device)\n",
    "    # Multiply each theta by each position using the outer product.\n",
    "    # Shape: (Seq_Len) outer_product* (Head_Dim / 2) -> (Seq_Len, Head_Dim / 2)\n",
    "    freqs = torch.outer(m, theta).float()\n",
    "    # We can compute complex numbers in the polar form c = R * exp(m * theta), where R = 1 as follows:\n",
    "    # (Seq_Len, Head_Dim / 2) -> (Seq_Len, Head_Dim / 2)\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_complex\n",
    "\n",
    "def apply_rotary_embeddings(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
    "    # Separate the last dimension pairs of two values, representing the real and imaginary parts of the complex number\n",
    "    # Two consecutive values will become a single complex number\n",
    "    # (B, Seq_Len, H, Head_Dim) -> (B, Seq_Len, H, Head_Dim/2)\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    # Reshape the freqs_complex tensor to match the shape of the x_complex tensor. So we need to add the batch dimension and the head dimension\n",
    "    # (Seq_Len, Head_Dim/2) --> (1, Seq_Len, 1, Head_Dim/2)\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "    # Multiply each complex number in the x_complex tensor by the corresponding complex number in the freqs_complex tensor\n",
    "    # Which results in the rotation of the complex number as shown in the Figure 1 of the paper\n",
    "    # (B, Seq_Len, H, Head_Dim/2) * (1, Seq_Len, 1, Head_Dim/2) = (B, Seq_Len, H, Head_Dim/2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "    # Convert the complex number back to the real number\n",
    "    # (B, Seq_Len, H, Head_Dim/2) -> (B, Seq_Len, H, Head_Dim/2, 2)\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "    # (B, Seq_Len, H, Head_Dim/2, 2) -> (B, Seq_Len, H, Head_Dim)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    return x_out.type_as(x).to(device)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # The gamma parameter\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x: torch.Tensor):\n",
    "        # (B, Seq_Len, Dim) * (B, Seq_Len, 1) = (B, Seq_Len, Dim)\n",
    "        # rsqrt: 1 / sqrt(x)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # (Dim) * (B, Seq_Len, Dim) = (B, Seq_Len, Dim)\n",
    "        return self.weight * self._norm(x.float()).type_as(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.attention = SelfAttention(args)\n",
    "        self.feed_forward = FeedForward(args)\n",
    "\n",
    "        # Normalization BEFORE the attention block\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        # Normalization BEFORE the feed forward block\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
    "        # (B, Seq_Len, Dim) + (B, Seq_Len, Dim) --> (B, Seq_Len, Dim)\n",
    "        h = x + self.attention.forward(\n",
    "            self.attention_norm(x), start_pos, freqs_complex\n",
    "        )\n",
    "        # (B, Seq_Len, Dim) + (B, Seq_Len, Dim) --> (B, Seq_Len, Dim)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out\n",
    "    \n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        # (B, Seq_Len, N_KV_Heads, 1, Head_Dim)\n",
    "        x[:, :, :, None, :]\n",
    "        # (B, Seq_Len, N_KV_Heads, N_Rep, Head_Dim)\n",
    "        .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "        # (B, Seq_Len, N_KV_Heads * N_Rep, Head_Dim)\n",
    "        .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Indicates the number of heads for the Keys and Values\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # Indicates the number of heads for the Queries\n",
    "        self.n_heads_q = args.n_heads\n",
    "        # Indicates how many times the Keys and Values should be repeated\n",
    "        self.n_rep = self.n_heads_q // self.n_kv_heads\n",
    "        # Indicates the dimension of each head, that is, the part of the embedding that each head will be responsible for\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "        self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_complex: torch.Tensor\n",
    "    ):\n",
    "        batch_size, seq_len, _ = x.shape  # (B, 1, Dim)\n",
    "\n",
    "        # (B, 1, Dim) -> (B, 1, H_Q * Head_Dim)\n",
    "        xq = self.wq(x)\n",
    "        # (B, 1, Dim) -> (B, 1, H_KV * Head_Dim)\n",
    "        xk = self.wk(x)\n",
    "        # (B, 1, Dim) -> (B, 1, H_KV * Head_Dim)\n",
    "        xv = self.wv(x)\n",
    "\n",
    "        # (B, 1, H_Q * Head_Dim) -> (B, 1, H_Q, Head_Dim)\n",
    "        xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
    "        # (B, 1, H_KV * Head_Dim) -> (B, 1, H_KV, Head_Dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        # (B, 1, H_KV * Head_Dim) -> (B, 1, H_KV, Head_Dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # (B, 1, H_Q, Head_Dim) --> (B, 1, H_Q, Head_Dim)\n",
    "        xq = apply_rotary_embeddings(xq, freqs_complex, device=x.device)\n",
    "        # (B, 1, H_KV, Head_Dim) --> (B, 1, H_KV, Head_Dim)\n",
    "        xk = apply_rotary_embeddings(xk, freqs_complex, device=x.device)\n",
    "\n",
    "        # Replace the entry in the cache\n",
    "        self.cache_k[:batch_size, start_pos : start_pos + seq_len] = xk\n",
    "        self.cache_v[:batch_size, start_pos : start_pos + seq_len] = xv\n",
    "\n",
    "        # (B, Seq_Len_KV, H_KV, Head_Dim)\n",
    "        keys = self.cache_k[:batch_size, : start_pos + seq_len]\n",
    "        # (B, Seq_Len_KV, H_KV, Head_Dim)\n",
    "        values = self.cache_v[:batch_size, : start_pos + seq_len]\n",
    "\n",
    "        # Since every group of Q shares the same K and V heads, just repeat the K and V heads for every Q in the same group.\n",
    "\n",
    "        # (B, Seq_Len_KV, H_KV, Head_Dim) --> (B, Seq_Len_KV, H_Q, Head_Dim)\n",
    "        keys = repeat_kv(keys, self.n_rep)\n",
    "        # (B, Seq_Len_KV, H_KV, Head_Dim) --> (B, Seq_Len_KV, H_Q, Head_Dim)\n",
    "        values = repeat_kv(values, self.n_rep)\n",
    "\n",
    "        # (B, 1, H_Q, Head_Dim) -> (B, H_Q, 1, Head_Dim)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        # (B, Seq_Len_KV, H_Q, Head_Dim) -> (B, H_Q, Seq_Len_KV, Head_Dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        # (B, Seq_Len_KV, H_Q, Head_Dim) -> (B, H_Q, Seq_Len_KV, Head_Dim)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # (B, H_Q, 1, Head_Dim) @ (B, H_Q, Head_Dim, Seq_Len_KV) -> (B, H_Q, 1, Seq_Len_KV)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        # (B, H_Q, 1, Seq_Len_KV) -> (B, H_Q, 1, Seq_Len_KV)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "\n",
    "        # (B, H_Q, 1, Seq_Len) @ (B, H_Q, Seq_Len_KV, Head_Dim) -> (B, H_Q, 1, Head_Dim)\n",
    "        output = torch.matmul(scores, values)\n",
    "        # (B, H_Q, 1, Head_Dim) -> (B, 1, H_Q, Head_Dim) -> (B, 1, Dim)\n",
    "        output = (output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1))\n",
    "        return self.wo(output) # (B, 1, Dim) -> (B, 1, Dim)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        args: ModelArgs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = 4 * args.dim\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        if args.ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
    "        # Round the hidden_dim to the nearest multiple of the multiple_of parameter\n",
    "        hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
    "        self.w3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        swish = F.silu(self.w1(x))\n",
    "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x_V = self.w3(x)\n",
    "        # (B, Seq_Len, Hidden_Dim) * (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x = swish * x_V\n",
    "        # (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Dim)\n",
    "        x = self.w2(x)\n",
    "        return x\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert args.vocab_size > 0, \"You must provide the vocabulary size.\"\n",
    "        self.args = args\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.n_layers = args.n_layers\n",
    "        self.tok_embeddings = nn.Embedding(self.vocab_size, args.dim)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(self.args.n_layers):\n",
    "            self.layers.append(EncoderBlock(args))\n",
    "\n",
    "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.output = nn.Linear(args.dim, self.vocab_size, bias=False)\n",
    "\n",
    "        self.freqs_complex = precompute_theta_pos_frequencies(self.args.dim // self.args.n_heads, self.args.max_seq_len * 2, device=self.args.device)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        # B, Seq-len\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        assert seq_len == 1, \"The sequence length must be one.\"\n",
    "\n",
    "        h = self.tok_embeddings(tokens)\n",
    "\n",
    "        freq_complex = self.freqs_complex[start_pos:start_pos + seq_len]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freq_complex)\n",
    "\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code the inference algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMA:\n",
    "\n",
    "    def __init__(self, model: Transformer, tokenizer: SentencePieceProcessor, model_args: ModelArgs):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.args = model_args\n",
    "\n",
    "    @staticmethod\n",
    "    def build(checkpoints_dir: str, tokenizer_path: str, load_model: bool, max_seq_len: int, max_batch_size: int, device: str):\n",
    "        prev_time = time.time()\n",
    "        if load_model:\n",
    "            checkpoints = sorted(Path(checkpoints_dir).glob(\"*.pth\"))\n",
    "            assert len(checkpoints) > 0, f\"no checkpoint files found in {checkpoints_dir}\"\n",
    "            ckpt_path = checkpoints[0]\n",
    "            print(f'Loading checkpoint \"{ckpt_path}\"')\n",
    "            checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "            print(f\"Loaded checkpoint in {time.time() - prev_time:.2f}s\")\n",
    "            prev_time = time.time()\n",
    "        with open(Path(checkpoints_dir) / \"params.json\", \"r\") as f:\n",
    "            params = json.loads(f.read())\n",
    "\n",
    "        model_args: ModelArgs = ModelArgs(\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "            device=device,\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        tokenizer = SentencePieceProcessor()\n",
    "        tokenizer.load(tokenizer_path)\n",
    "        model_args.vocab_size = tokenizer.vocab_size()\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "        else:\n",
    "            torch.set_default_tensor_type(torch.BFloat16Tensor)\n",
    "        \n",
    "        model = Transformer(model_args).to(device)\n",
    "\n",
    "        if load_model:\n",
    "            # The only unmatched key in the checkpoint is rope.freqs. Remove it\n",
    "            del checkpoint['rope.freqs']\n",
    "            model.load_state_dict(checkpoint, strict=True)\n",
    "            print(f\"Loaded state dict in {time.time() - prev_time:.2f}s\")\n",
    "        \n",
    "        return LLaMA(model, tokenizer, model_args)\n",
    "\n",
    "    def text_completion(self, prompts: list[str], temperature: float = 0.6, top_p: float = 0.9, max_gen_len: Optional[int] = None):\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.args.max_seq_len - 1\n",
    "        # Convert each prompt into tokens\n",
    "        prompt_tokens = [self.tokenizer.encode(prompt, out_type=int, add_bos=True, add_eos=False) for prompt in prompts]\n",
    "        # Make sure the batch size is not too large\n",
    "        batch_size = len(prompt_tokens)\n",
    "        assert batch_size <= self.args.max_batch_size, f\"batch size must be less than or equal to {self.args.max_batch_size}\"\n",
    "        max_prompt_len = max(len(prompt) for prompt in prompt_tokens)\n",
    "        # Make sure the prompt length is not larger than the maximum sequence length\n",
    "        assert max_prompt_len <= self.args.max_seq_len, f\"prompt length must be less than or equal to {self.args.max_seq_len}\"\n",
    "        total_len = min(self.args.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        # Create the list that will contain the generated tokens, along with the initial prompt tokens\n",
    "        pad_id = self.tokenizer.pad_id()\n",
    "        tokens = torch.full((batch_size, total_len), pad_id, dtype=torch.long, device=device)\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            # Populate the initial tokens with the prompt tokens\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
    "        \n",
    "        eos_reached = torch.tensor([False] * batch_size, device=device)\n",
    "        prompt_tokens_mask = tokens != pad_id # True if the token is a prompt token, False otherwise\n",
    "        cur_iterator = tqdm(range(1, total_len), desc=\"Generating tokens\")\n",
    "        for cur_pos in cur_iterator:\n",
    "            with torch.no_grad():\n",
    "                logits = self.model.forward(tokens[:, cur_pos-1:cur_pos], cur_pos)\n",
    "            if temperature > 0:\n",
    "                # The temperature is applied before the softmax\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = self._sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                # Greedily select the token with the max probability\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # Only replace token if it is a padding token\n",
    "            next_token = torch.where(prompt_tokens_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            # EOS is reached only if we found an EOS token for a padding position\n",
    "            eos_reached |= (~prompt_tokens_mask[:, cur_pos]) & (next_token == self.tokenizer.eos_id)\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        out_tokens = []\n",
    "        out_text = []\n",
    "        for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):\n",
    "            # Cut to the EOS token, if present\n",
    "            if self.tokenizer.eos_id in current_prompt_tokens:\n",
    "                eos_idx = current_prompt_tokens.index(self.tokenizer.eos_id)\n",
    "                current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
    "            out_tokens.append(current_prompt_tokens)\n",
    "            out_text.append(self.tokenizer.decode(current_prompt_tokens))\n",
    "        return (out_tokens, out_text)\n",
    "    \n",
    "    def _sample_top_p(self, probs, p):\n",
    "        # (B, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "        # (B, vocab_size)\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "        # (B, vocab_size)\n",
    "        # (Substracting \"probs_sort\" shifts the cumulative sum by 1 position to the right before masking)\n",
    "        mask = probs_sum - probs_sort > p \n",
    "        # Zero out all the probabilities of tokens that are not selected by the Top P\n",
    "        probs_sort[mask] = 0.0 \n",
    "        # Redistribute the probabilities so that they sum up to 1.\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        # Sample a token (its index) from the top p distribution\n",
    "        next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "        # Get the token position in the vocabulary corresponding to the sampled index\n",
    "        next_token = torch.gather(probs_idx, -1, next_token) \n",
    "        return next_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint \"../llama-2-7b/consolidated.00.pth\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/c32_bthx48jd9m2ym5m3tnpw0000j7/T/ipykernel_74692/3750953715.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint in 17.61s\n",
      "Loaded state dict in 82.34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tokens: 100%|██████████| 114/114 [1:05:02<00:00, 34.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simply put, the theory of relativity states that 1) the laws of physics are the same for all non-accelerating observers, and 2) the speed of light is the same for all observers, regardless of their relative speed of motion. Powered by Create your own unique website with customizable templates. Einstein's Theory of Relativity. In fact, they are both based on the principle that the speed of light is the same for all observers. Einstein's theory of relativity has been criticized for\n",
      "--------------------------------------------------\n",
      "If Google was an Italian company founded in Milan, it would be called Google Italia. com, you will find all the information you need to know about Google.\n",
      "I'm not a lawyer, but I've been told that this is illegal in Italy.\n",
      "Google Italia is an Italian search engine that provides search results in Italian.\n",
      "If you are in Italy and you have a Google account, you can use it to search the web, but you will not be able to use Google to search for Italian websites.\n",
      "If Google was a company that was founded\n",
      "--------------------------------------------------\n",
      "Translate English to French:\n",
      "    \n",
      "    sea otter => loutre de mer\n",
      "    peppermint => menthe poivrée\n",
      "    plush girafe => girafe peluche\n",
      "    cheese => fromage\n",
      "    egg => œuf\n",
      "    tiger => tigre\n",
      "    chocolate => chocolat\n",
      "    banana => banane\n",
      "    penguin => pinguin\n",
      "    panda => panda\n",
      "    rhinoceros => rhinocéros\n",
      "   \n",
      "--------------------------------------------------\n",
      "Tell me if the following person is actually Doraemon disguised as human:\n",
      "    Name: Umar Jamil\n",
      "    Decision: \n",
      "    1. He is Doraemon\n",
      "    2. He is not Doraemon\n",
      "\n",
      "---\n",
      "\n",
      "### Doraemon\n",
      "\n",
      "He is a robotic cat that comes from the future to help Nobita Nobi, a lazy and \n",
      "useless boy. Doraemon has a 22nd century Japanese high-tech robotic cat.\n",
      "\n",
      "---\n",
      "\n",
      "###\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "prompts = [\n",
    "    \"Simply put, the theory of relativity states that \",\n",
    "    \"If Google was an Italian company founded in Milan, it would\",\n",
    "    # Few shot promt\n",
    "    \"\"\"Translate English to French:\n",
    "    \n",
    "    sea otter => loutre de mer\n",
    "    peppermint => menthe poivrée\n",
    "    plush girafe => girafe peluche\n",
    "    cheese =>\"\"\",\n",
    "    # Zero shot prompt\n",
    "    \"\"\"Tell me if the following person is actually Doraemon disguised as human:\n",
    "    Name: Umar Jamil\n",
    "    Decision: \n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "model = LLaMA.build(\n",
    "    checkpoints_dir='../llama-2-7b/',\n",
    "    tokenizer_path='../llama-2-7b/tokenizer.model',\n",
    "    load_model=True,\n",
    "    max_seq_len=1024,\n",
    "    max_batch_size=len(prompts),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "out_tokens, out_texts = (model.text_completion(prompts, max_gen_len=64))\n",
    "assert len(out_texts) == len(prompts)\n",
    "for i in range(len(out_texts)):\n",
    "    print(f'{out_texts[i]}')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU](https://www.youtube.com/watch?v=Mn_9W1nCFLo)\n",
    "- [Coding LLaMA 2 from scratch in PyTorch - KV Cache, Grouped Query Attention, Rotary PE, RMSNorm](https://www.youtube.com/watch?v=oM4VmoabDAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
