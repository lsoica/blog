{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsoica/blog/blob/main/assets/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTOLgsbN69-P"
      },
      "source": [
        "# RNN\n",
        "\n",
        "RNNs are a type of neural network that are designed to process sequential data. Allows previous outputs to be used as inputs while having hidden states.\n",
        "\n",
        "![RNN](../assets/images/rnn.png)\n",
        "\n",
        "[RNN effectiveness](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "\n",
        "RNNs allows us to process sequential data.\n",
        "\n",
        "![RNN operates on sequences](../assets/images/rnn-operates-on-sequences.png)\n",
        "\n",
        "## Let's build a RNN to classify names by origin\n",
        "\n",
        "- We treat each name as a sequence of characters.\n",
        "- Each character is represented by a one-hot vector. A tensor of fixed size where only one element is 1 and the rest are 0s. Each character is uniquely identified by its index.\n",
        "- We have 57 characters in total in our vocabulary so the tensor size is 57.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (2.5.1)\n",
            "Requirement already satisfied: matplotlib in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (3.9.2)\n",
            "Requirement already satisfied: filelock in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: setuptools in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.23 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/LSoica/work/AI/blog/.venv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install torch matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import unicodedata\n",
        "import string\n",
        "import glob\n",
        "\n",
        "import torch\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-03 13:00:09--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.224.14.23, 13.224.14.58, 13.224.14.44, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.224.14.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2,7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   2,75M  2,41MB/s    in 1,1s    \n",
            "\n",
            "2024-12-03 13:00:11 (2,41 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ]
        }
      ],
      "source": [
        "# https://download.pytorch.org/tutorial/data.zip\n",
        "!rm -rf data.zip* data/\n",
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip\n",
        "!rm -rf data.zip*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Some utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# alphabet small + capital letters + \" .,;'\"\n",
        "ALL_LETTERS = string.ascii_letters + \" .,;'\"\n",
        "N_LETTERS = len(ALL_LETTERS)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in ALL_LETTERS\n",
        "    )\n",
        "\n",
        "def load_data():\n",
        "    # Build the category_lines dictionary, a list of names per language\n",
        "    category_lines = {}\n",
        "    all_categories = []\n",
        "    \n",
        "    def find_files(path):\n",
        "        return glob.glob(path)\n",
        "    \n",
        "    # Read a file and split into lines\n",
        "    def read_lines(filename):\n",
        "        lines = io.open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "        return [unicode_to_ascii(line) for line in lines]\n",
        "    \n",
        "    for filename in find_files('data/names/*.txt'):\n",
        "        category = os.path.splitext(os.path.basename(filename))[0]\n",
        "        all_categories.append(category)\n",
        "        \n",
        "        lines = read_lines(filename)\n",
        "        category_lines[category] = lines\n",
        "        \n",
        "    return category_lines, all_categories\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "To represent a single letter, we use a “one-hot vector” of \n",
        "size <1 x n_letters>. A one-hot vector is filled with 0s\n",
        "except for a 1 at index of the current letter, e.g. \"b\" = <0 1 0 0 0 ...>.\n",
        "\n",
        "To make a word we join a bunch of those into a\n",
        "2D matrix <line_length x 1 x n_letters>.\n",
        "\n",
        "That extra 1 dimension is because PyTorch assumes\n",
        "everything is in batches - we’re just using a batch size of 1 here.\n",
        "\"\"\"\n",
        "\n",
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letter_to_index(letter):\n",
        "    return ALL_LETTERS.find(letter)\n",
        "\n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letter_to_tensor(letter):\n",
        "    tensor = torch.zeros(1, N_LETTERS)\n",
        "    tensor[0][letter_to_index(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def line_to_tensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, N_LETTERS)\n",
        "    for i, letter in enumerate(line):\n",
        "        tensor[i][0][letter_to_index(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def random_training_example(category_lines, all_categories):\n",
        "    \n",
        "    def random_choice(a):\n",
        "        random_idx = random.randint(0, len(a) - 1)\n",
        "        return a[random_idx]\n",
        "    \n",
        "    category = random_choice(all_categories)\n",
        "    line = random_choice(category_lines[category])\n",
        "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
        "    line_tensor = line_to_tensor(line)\n",
        "    return category, line, category_tensor, line_tensor\n",
        "\n",
        "def category_from_output(output, all_categories):\n",
        "    category_idx = torch.argmax(output).item()\n",
        "    return all_categories[category_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'\n",
            "57\n",
            "Slusarski\n",
            "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0.]])\n",
            "torch.Size([5, 1, 57])\n"
          ]
        }
      ],
      "source": [
        "print(ALL_LETTERS)\n",
        "print(N_LETTERS)\n",
        "print(unicode_to_ascii('Ślusàrski'))\n",
        "\n",
        "category_lines, all_categories = load_data()\n",
        "print(category_lines['Italian'][:5])\n",
        "\n",
        "print(letter_to_tensor('J')) # [1, 57]\n",
        "print(line_to_tensor('Jones').size()) # [5, 1, 57]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the network\n",
        "\n",
        "![RNN model architecture](../assets/images/rnn-model-architecture.png)\n",
        "\n",
        " - input and hidden states are concatenated\n",
        " - input to hidden is a linear layer. it's input size is input size + hidden size as the two are concatenated\n",
        " - input to output is a linear layer. it's input size is input size + hidden size as the two are concatenated\n",
        " - torch.cat is used to concatenate the input and hidden states. What id does is that it takes two tensors and concatenates them along a specified dimension. In this case, it concatenates the input and hidden states along dimension 1 (the second dimension).\n",
        " - softmax is applied to the output to get a probability distribution over the categories\n",
        " - optimizer is stochastic gradient descent: https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
        " - criterion is negative log likelihood loss: https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "class RNN(nn.Module):\n",
        "    # implement RNN from scratch rather than using nn.RNN\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "    def forward(self, input_tensor, hidden_tensor):\n",
        "        combined = torch.cat((input_tensor, hidden_tensor), 1)\n",
        "        \n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "    \n",
        "n_hidden = 128\n",
        "n_categories = len(all_categories)\n",
        "rnn = RNN(N_LETTERS, n_hidden, n_categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Abl', 'Adsit', 'Ajdrna', 'Alt', 'Antonowitsch']\n",
            "Number of categories: 18\n",
            "Number of letters: 57\n",
            "Hidden size: 128\n"
          ]
        }
      ],
      "source": [
        "print(category_lines[all_categories[0]][:5])\n",
        "print(f\"Number of categories: {n_categories}\")\n",
        "print(f\"Number of letters: {N_LETTERS}\")\n",
        "print(f\"Hidden size: {n_hidden}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's do one forward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input sequence size: 6\n",
            "torch.Size([1, 18])\n",
            "Russian\n"
          ]
        }
      ],
      "source": [
        "input_tensor = line_to_tensor('Albert')\n",
        "hidden_tensor = rnn.init_hidden()\n",
        "print(f\"Input sequence size: {input_tensor.size()[0]}\")\n",
        "for i in range(input_tensor.size()[0]):\n",
        "    output, hidden_tensor = rnn(input_tensor[i], hidden_tensor)\n",
        "print(output.shape)\n",
        "guess = category_from_output(output, all_categories)\n",
        "print(guess)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5000 5.0 2.5811 Marik / Czech CORRECT\n",
            "10000 10.0 2.2606 Strohkirch / Polish WRONG (German)\n",
            "15000 15.0 3.3690 Salomon / Irish WRONG (Polish)\n",
            "20000 20.0 1.8265 Araujo / Portuguese WRONG (Spanish)\n",
            "25000 25.0 2.2065 Picasso / Italian WRONG (Spanish)\n",
            "30000 30.0 0.4089 Ozawa / Japanese CORRECT\n",
            "35000 35.0 2.0963 Kudrna / Japanese WRONG (Czech)\n",
            "40000 40.0 0.5560 Domhnall / Irish CORRECT\n",
            "45000 45.0 2.4638 Hubbard / Arabic WRONG (English)\n",
            "50000 50.0 0.6749 Nozaki / Japanese CORRECT\n",
            "55000 55.00000000000001 0.1868 Pispinis / Greek CORRECT\n",
            "60000 60.0 4.9780 Park  / Polish WRONG (Korean)\n",
            "65000 65.0 5.8802 Forer / Portuguese WRONG (Russian)\n",
            "70000 70.0 1.0734 Meisner / German CORRECT\n",
            "75000 75.0 0.9770 Freitas / Portuguese CORRECT\n",
            "80000 80.0 4.1851 Gesse / Scottish WRONG (Russian)\n",
            "85000 85.0 2.3861 Kurtz / German WRONG (Czech)\n",
            "90000 90.0 0.3525 Malouf / Arabic CORRECT\n",
            "95000 95.0 1.9145 Chung / Korean WRONG (Vietnamese)\n",
            "100000 100.0 0.7894 Jung  / Korean CORRECT\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "learning_rate = 0.005\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
        "\n",
        "def train(line_tensor, category_tensor):\n",
        "    hidden = rnn.init_hidden()\n",
        "    \n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        output, hidden = rnn(line_tensor[i], hidden)\n",
        "        \n",
        "    loss = criterion(output, category_tensor)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    return output, loss.item()\n",
        "\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "plot_steps, print_steps = 1000, 5000\n",
        "n_iters = 100000\n",
        "for i in range(n_iters):\n",
        "    category, line, category_tensor, line_tensor = random_training_example(category_lines, all_categories)\n",
        "    \n",
        "    output, loss = train(line_tensor, category_tensor)\n",
        "    current_loss += loss \n",
        "    \n",
        "    if (i+1) % plot_steps == 0:\n",
        "        all_losses.append(current_loss / plot_steps)\n",
        "        current_loss = 0\n",
        "        \n",
        "    if (i+1) % print_steps == 0:\n",
        "        guess = category_from_output(output, all_categories)\n",
        "        correct = \"CORRECT\" if guess == category else f\"WRONG ({category})\"\n",
        "        print(f\"{i+1} {(i+1)/n_iters*100} {loss:.4f} {line} / {guess} {correct}\")\n",
        "        \n",
        "    \n",
        "plt.figure()\n",
        "plt.plot(all_losses)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "> Albert\n",
            "English\n",
            "\n",
            "> Albert\n",
            "English\n"
          ]
        }
      ],
      "source": [
        "def predict(input_line):\n",
        "    print(f\"\\n> {input_line}\")\n",
        "    with torch.no_grad():\n",
        "        line_tensor = line_to_tensor(input_line)\n",
        "        \n",
        "        hidden = rnn.init_hidden()\n",
        "    \n",
        "        for i in range(line_tensor.size()[0]):\n",
        "            output, hidden = rnn(line_tensor[i], hidden)\n",
        "        \n",
        "        guess = category_from_output(output, all_categories)\n",
        "        print(guess)\n",
        "\n",
        "\n",
        "while True:\n",
        "    sentence = input(\"Input:\")\n",
        "    if sentence == \"quit\":\n",
        "        break\n",
        "    \n",
        "    predict(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "[PyTorch RNN Tutorial - Name Classification Using A Recurrent Neural Net](https://www.youtube.com/watch?v=WEV61GmmPrk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's build a RNN to classify images\n",
        "\n",
        "This time we will use the PyTorch implementation of RNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 7.15MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 226kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.44MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 10.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Epoch [1/2], Step [100/600], Loss: 1.0227\n",
            "Epoch [1/2], Step [200/600], Loss: 0.6099\n",
            "Epoch [1/2], Step [300/600], Loss: 0.3354\n",
            "Epoch [1/2], Step [400/600], Loss: 0.5249\n",
            "Epoch [1/2], Step [500/600], Loss: 0.5032\n",
            "Epoch [1/2], Step [600/600], Loss: 0.2073\n",
            "Epoch [2/2], Step [100/600], Loss: 0.3670\n",
            "Epoch [2/2], Step [200/600], Loss: 0.2080\n",
            "Epoch [2/2], Step [300/600], Loss: 0.3060\n",
            "Epoch [2/2], Step [400/600], Loss: 0.2490\n",
            "Epoch [2/2], Step [500/600], Loss: 0.1272\n",
            "Epoch [2/2], Step [600/600], Loss: 0.2224\n",
            "Accuracy of the network on the 10000 test images: 93.89 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters \n",
        "# input_size = 784 # 28x28\n",
        "num_classes = 10\n",
        "num_epochs = 2\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_size = 28\n",
        "sequence_length = 28\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "\n",
        "# MNIST dataset \n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),  \n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "# Fully connected neural network with one hidden layer\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # -> x needs to be: (batch_size, seq, input_size)\n",
        "        \n",
        "        # or:\n",
        "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Set initial hidden states (and cell states for LSTM)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "        \n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        \n",
        "        # Forward propagate RNN\n",
        "        out, _ = self.rnn(x, h0)  \n",
        "        # or:\n",
        "        #out, _ = self.lstm(x, (h0,c0))  \n",
        "        \n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        # out: (n, 28, 128)\n",
        "        \n",
        "        # Decode the hidden state of the last time step\n",
        "        out = out[:, -1, :]\n",
        "        # out: (n, 128)\n",
        "         \n",
        "        out = self.fc(out)\n",
        "        # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# Train the model\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # origin shape: [N, 1, 28, 28]\n",
        "        # resized: [N, 28, 28]\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Now replace RNN with GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [100/600], Loss: 0.7604\n",
            "Epoch [1/2], Step [200/600], Loss: 0.2869\n",
            "Epoch [1/2], Step [300/600], Loss: 0.1475\n",
            "Epoch [1/2], Step [400/600], Loss: 0.4941\n",
            "Epoch [1/2], Step [500/600], Loss: 0.1315\n",
            "Epoch [1/2], Step [600/600], Loss: 0.1536\n",
            "Epoch [2/2], Step [100/600], Loss: 0.0720\n",
            "Epoch [2/2], Step [200/600], Loss: 0.1538\n",
            "Epoch [2/2], Step [300/600], Loss: 0.2200\n",
            "Epoch [2/2], Step [400/600], Loss: 0.0656\n",
            "Epoch [2/2], Step [500/600], Loss: 0.0800\n",
            "Epoch [2/2], Step [600/600], Loss: 0.0541\n",
            "Accuracy of the network on the 10000 test images: 97.56 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters \n",
        "# input_size = 784 # 28x28\n",
        "num_classes = 10\n",
        "num_epochs = 2\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_size = 28\n",
        "sequence_length = 28\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "\n",
        "# MNIST dataset \n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),  \n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "# Fully connected neural network with one hidden layer\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # -> x needs to be: (batch_size, seq, input_size)\n",
        "        \n",
        "        # or:\n",
        "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Set initial hidden states (and cell states for LSTM)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "        \n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        \n",
        "        # Forward propagate RNN\n",
        "        out, _ = self.gru(x, h0)  \n",
        "        # or:\n",
        "        #out, _ = self.lstm(x, (h0,c0))  \n",
        "        \n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        # out: (n, 28, 128)\n",
        "        \n",
        "        # Decode the hidden state of the last time step\n",
        "        out = out[:, -1, :]\n",
        "        # out: (n, 128)\n",
        "         \n",
        "        out = self.fc(out)\n",
        "        # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# Train the model\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # origin shape: [N, 1, 28, 28]\n",
        "        # resized: [N, 28, 28]\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now replace GRU with LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [100/600], Loss: 0.6295\n",
            "Epoch [1/2], Step [200/600], Loss: 0.3988\n",
            "Epoch [1/2], Step [300/600], Loss: 0.3129\n",
            "Epoch [1/2], Step [400/600], Loss: 0.2452\n",
            "Epoch [1/2], Step [500/600], Loss: 0.1903\n",
            "Epoch [1/2], Step [600/600], Loss: 0.0380\n",
            "Epoch [2/2], Step [100/600], Loss: 0.1356\n",
            "Epoch [2/2], Step [200/600], Loss: 0.1113\n",
            "Epoch [2/2], Step [300/600], Loss: 0.2232\n",
            "Epoch [2/2], Step [400/600], Loss: 0.0467\n",
            "Epoch [2/2], Step [500/600], Loss: 0.0519\n",
            "Epoch [2/2], Step [600/600], Loss: 0.1000\n",
            "Accuracy of the network on the 10000 test images: 97.64 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters \n",
        "# input_size = 784 # 28x28\n",
        "num_classes = 10\n",
        "num_epochs = 2\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_size = 28\n",
        "sequence_length = 28\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "\n",
        "# MNIST dataset \n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),  \n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "# Fully connected neural network with one hidden layer\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # -> x needs to be: (batch_size, seq, input_size)\n",
        "        \n",
        "        # or:\n",
        "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Set initial hidden states (and cell states for LSTM)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "        \n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        \n",
        "        # Forward propagate RNN\n",
        "        out, _ = self.lstm(x, (h0,c0))  \n",
        "        # or:\n",
        "        #out, _ = self.lstm(x, (h0,c0))  \n",
        "        \n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        # out: (n, 28, 128)\n",
        "        \n",
        "        # Decode the hidden state of the last time step\n",
        "        out = out[:, -1, :]\n",
        "        # out: (n, 128)\n",
        "         \n",
        "        out = self.fc(out)\n",
        "        # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# Train the model\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # origin shape: [N, 1, 28, 28]\n",
        "        # resized: [N, 28, 28]\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "5-probability.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
