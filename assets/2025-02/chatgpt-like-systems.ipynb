{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining - Building the dataset\n",
    "\n",
    "This is the pre-training step: download and process the internet data. A reference implementation is available on [HuggingFaceFW/blogpost-fineweb-v1](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1).\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "We could represent the text as a sequence of zeros and ones, but this would require a huge number of parameters as we only have two symbols. Instead, we use a technique called tokenization, which is a way to represent the text as a sequence of tokens. It turns out that the optimal number of tokens is around 100,000 (GPT-4).\n",
    "\n",
    "See [(https://tiktokenizer.vercel.app/)](https://tiktokenizer.vercel.app/)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
