{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek Paper\n",
    "\n",
    "[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "The algorithm used by DeepSeek is GRPO. Traditiinally, PPO is used for RLHF.\n",
    "\n",
    "GRPO - Group Relative Policy Optimization\n",
    "\n",
    "## How\n",
    "\n",
    "We have a dataset of preferences. We ask the model to generate responses to the prompts. We then use the preferences to train the model.\n",
    "\n",
    "Reward Model: Usually a reward model is used to score the quality of the response. In DeepSeek, they use a rule-based reward system.\n",
    "\n",
    "\n",
    "Policy Gradient Optimization: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning from Human Feedback\n",
    "\n",
    "* Agent: This is the actor taking actions.\n",
    "* State: This is the current state of the model.\n",
    "* Action: This is the action taken by the model.\n",
    "* Reward: This is the reward received by the model.\n",
    "* Policy: A policy rules how the agent behaves given the state it is in.\n",
    "\n",
    "$$\n",
    "a_t = \\pi(.|s_t)\n",
    "$$\n",
    "\n",
    "with what probability the agent will take action $a_t$ given the state $s_t$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward model for language models\n",
    "\n",
    "For each question and answer pair, generate a reward $r(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
